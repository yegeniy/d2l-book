{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yegeniy/d2l-book/blob/master/ds_code/pytorch-backward-gradient-examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEVWYrlVClQD"
      },
      "source": [
        "Link to post: https://medium.com/@zhang_yang/the-gradient-argument-in-pytorchs-backward-function-explained-by-examples-68f266950c29"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB9jOymDClQE"
      },
      "source": [
        "# How does the \"gradient\" argument in Pytorch's \"backward\" function work - explained by examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dar3R0OYClQF"
      },
      "source": [
        "This post is some examples for the `gradient` argument in Pytorch's `backward` function. The math of `backward(gradient)` is explained in this [tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#gradients) and these threads ([thread-1](https://discuss.pytorch.org/t/gradient-argument-in-out-backward-gradient/12742), [thread-2](https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments)), along with some examples. Those were very helpful, but I wish there were more examples on how the numbers in the example correspond to the math, to help me more easily understand. I could not find many such examples so I will make some and write them here, so that I can look back when I forget this in two weeks.\n",
        "\n",
        "In the examples, I run code in torch, write down the math, and run the math in numpy, and show that the torch result matches the math/numpy result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "008GGRs-ClQG"
      },
      "source": [
        "Here's how Pytorch [tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#gradients) explains the math:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx2l0va7ClQG"
      },
      "source": [
        "[Insert picture]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1NY2TbsClQH"
      },
      "source": [
        "*We will make examples of $x$ and $y=f(x)$ (we omit the arrow-hats of $x$ and $y$ above), and manually calculate Jacobian $J$.*\n",
        "\n",
        "Pytorch tutorial goes on with the explanation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peWoGgZPClQH"
      },
      "source": [
        "[Insert picture]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWk431gJClQI"
      },
      "source": [
        "The above basically says: if you pass $v^T$ as the `gradient` argument, then `y.backward(gradient)` will give you not $J$ but $v^T \\cdot J$ as the result of `x.grad`.\n",
        "\n",
        "*We will make examples of $v^T$, calculate $v^T \\cdot J$ in numpy, and confirm that the result is the same as `x.grad` after calling `y.backward(gradient)` where `gradient` is $v^T$.*\n",
        "\n",
        "All good? Let's go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbMCltZAClQI",
        "outputId": "126df927-563b-4e42-f308-46f70175b5f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0.0.dev20181130\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "import numpy as np\n",
        "from torch import tensor\n",
        "from numpy import array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeTzpj1LClQK"
      },
      "source": [
        "## input is scalar, output is scalar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34DGd9bMClQK"
      },
      "source": [
        "First, a simple example where $x=1$ and $y = x^2$ are both scalar.\n",
        "\n",
        "In pytorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "BAk4ZvqmClQL",
        "outputId": "66e1f254-b5e0-4cef-cb19-c3fea63ce6fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x: tensor(1., requires_grad=True)\n",
            "y: tensor(1., grad_fn=<PowBackward0>)\n",
            "x.grad: tensor(2.)\n"
          ]
        }
      ],
      "source": [
        "x = tensor(1., requires_grad=True)\n",
        "print('x:', x)\n",
        "y = x**2\n",
        "print('y:', y)\n",
        "y.backward() # this is the same as y.backward(tensor(1.))\n",
        "print('x.grad:', x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdr8jMn-ClQL"
      },
      "source": [
        "Now manually calculate Jacobian $J$. In this case $x$ and $y$ are both scalar (each only has one component $x_1$ and $y_1$ respectively). And we have\n",
        "\n",
        "$$\n",
        "J = \\left(\\frac{\\partial y_1}{\\partial x_1}\\right) = \\left(\\frac{\\partial y}{\\partial x}\\right) = \\left(2x\\right)\n",
        "$$\n",
        "\n",
        "In numpy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7FEqIUmClQL",
        "outputId": "78c98bf2-0859-4361-8b92-f8fc3b962846"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "J: [[2.]]\n"
          ]
        }
      ],
      "source": [
        "x = x.detach().numpy()\n",
        "J = array([[2*x]])\n",
        "print('J:', J)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJnZCJn_ClQM"
      },
      "source": [
        "In this example, we did not pass the `gradient` argument to `backward()`, and this defaults to passing the value 1. As a reminder, $v^T$ is our `gradient` with value 1. We can confirm that $v^T \\cdot J$ gives the same result as `x.grad`. All good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zY0ljUXhClQM",
        "outputId": "846d2779-f34c-4fcb-f98c-d5be7c9ffe3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vᵀ: [[1]]\n",
            "vᵀ・J: [[2.]]\n"
          ]
        }
      ],
      "source": [
        "vᵀ = array([[1,]])\n",
        "print('vᵀ:', vᵀ)\n",
        "print('vᵀ・J:', vᵀ@J)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vSwiUo4ClQM"
      },
      "source": [
        "## input is scalar, output is scalar, non-default gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBoU5o4qClQM"
      },
      "source": [
        "We can keep everything else the same but pass a non-default `gradient` with the value 100 to `backward()` instead of the default value 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "qmf1ceM6ClQN",
        "outputId": "bb481d14-1bde-4b2d-fad0-b4e5f2525101"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x: tensor(1., requires_grad=True)\n",
            "y: tensor(1., grad_fn=<PowBackward0>)\n",
            "x.grad: tensor(200.)\n"
          ]
        }
      ],
      "source": [
        "x = tensor(1., requires_grad=True)\n",
        "print('x:', x)\n",
        "y = x**2\n",
        "print('y:', y)\n",
        "gradient_value = 100.\n",
        "y.backward(tensor(gradient_value))\n",
        "print('x.grad:', x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTFvUQRnClQN"
      },
      "source": [
        "This is the same as setting the value `100` for $v^T$, and we can see $v^T \\cdot J$ still matches `x.grad`. Still good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XonxfAqUClQN",
        "outputId": "55d9aa21-cffd-4535-a9d5-e50d33800019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "J: [[2.]]\n",
            "vᵀ: [[100.]]\n",
            "vᵀ・J: [[200.]]\n"
          ]
        }
      ],
      "source": [
        "x = x.detach().numpy()\n",
        "J = array([[2*x]])\n",
        "print('J:', J)\n",
        "\n",
        "vᵀ = array([[gradient_value,]])\n",
        "print('vᵀ:', vᵀ)\n",
        "print('vᵀ・J:', vᵀ@J)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj5ssA4vClQN"
      },
      "source": [
        "## input is vector, output is scalar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANUuFEF5ClQN"
      },
      "source": [
        "Now let's look at a more interesting example where $x=[x_1,x_2]=[1,2]$ is a vector and $y=\\sum x_i$ is a scalar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZwVGIvAClQO",
        "outputId": "3040ade7-9026-4faf-85c8-bde94075ca94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x: tensor([1., 2.], requires_grad=True)\n",
            "y: tensor(3., grad_fn=<AddBackward0>)\n",
            "x.grad: tensor([1., 1.])\n"
          ]
        }
      ],
      "source": [
        "x = tensor([1., 2.], requires_grad=True)\n",
        "print('x:', x)\n",
        "y = sum(x)\n",
        "print('y:', y)\n",
        "y.backward()\n",
        "print('x.grad:', x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEDlvtEGClQO"
      },
      "source": [
        "Now manually calculate Jacobian $J$. In this since $x$ is a vector with components $x_1$ and $x_2$, and $y=x_1+x_2$ is a scalar. We have\n",
        "\n",
        "$$\n",
        "J =  \n",
        "\\left( \\begin{array}{c}\n",
        "\\frac{\\partial y_1}{\\partial x_1}, \\frac{\\partial y_1}{\\partial x_2}\n",
        "\\end{array} \\right)\n",
        "=\n",
        "\\left( \\begin{array}{c} 1,1\n",
        "\\end{array} \\right)\n",
        "$$\n",
        "\n",
        "\n",
        "In numpy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REAsNWR6ClQO",
        "outputId": "36e12166-ca76-42a3-9c41-beacf806389e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "J:\n",
            "[[1 1]]\n"
          ]
        }
      ],
      "source": [
        "J = array([[1, 1]])\n",
        "print('J:')\n",
        "print(J)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsqVp4MNClQP"
      },
      "source": [
        "In this example, we did not pass the `gradient` argument to `backward()`, and this defaults to passing the value 1, i.e., $v^T$ has value 1. We can confirm that $v^T \\cdot J$ gives the same result as `x.grad`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhKE3LT3ClQP",
        "outputId": "9669148f-1376-442d-e3bd-14815157a518"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vᵀ: [[1]]\n",
            "vᵀ・J: [[1 1]]\n"
          ]
        }
      ],
      "source": [
        "vᵀ = array([[1]])\n",
        "print('vᵀ:', vᵀ)\n",
        "print('vᵀ・J:', vᵀ@J)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYmtLc_oClQP"
      },
      "source": [
        "## input is vector, output is scalar, non-default gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOYz2GSPClQP"
      },
      "source": [
        "We can keep everything else the same as above but pass a non-default `gradient` with the value 100 to `backward()` instead of the default value 1. Still, $x=[x_1,x_2]=[1,2]$ is a vector and $y=\\sum x_i$ is a scalar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKDwYCPIClQP",
        "outputId": "676758cb-ad9e-4d56-c2a1-677c655daf6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x: tensor([1., 2.], requires_grad=True)\n",
            "x.grad: tensor([100., 100.])\n"
          ]
        }
      ],
      "source": [
        "x = tensor([1., 2.], requires_grad=True)\n",
        "print('x:', x)\n",
        "y = sum(x)\n",
        "gradient_value = 100.\n",
        "y.backward(tensor(gradient_value))\n",
        "print('x.grad:', x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym6z2xBtClQQ"
      },
      "source": [
        "This is the same as setting the value 100 for $v^T$, and we can see $v^T \\cdot J$ still matches `x.grad`. Still good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKWhPsAFClQQ",
        "outputId": "2df92c56-0aa4-4da5-bd70-c42f16120042"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "J:\n",
            "[[1 1]]\n",
            "vᵀ: [[100.]]\n",
            "vᵀ・J: [[100. 100.]]\n"
          ]
        }
      ],
      "source": [
        "J = array([[1, 1]])\n",
        "print('J:')\n",
        "print(J)\n",
        "\n",
        "\n",
        "vᵀ = array([[gradient_value,]])\n",
        "print('vᵀ:', vᵀ)\n",
        "print('vᵀ・J:', vᵀ@J)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf5mKhpUClQQ"
      },
      "source": [
        "## input is vector, output is vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK74N4NvClQR"
      },
      "source": [
        "Now let's look at an example where both $x=[x_1,x_2]=[1,2]$ and $y=3x^2$ are vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovlIZESoClQR",
        "outputId": "850a4907-c4ab-4a13-9ef2-52950d6ccc6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x: tensor([1., 2.], requires_grad=True)\n",
            "y: tensor([ 3., 12.], grad_fn=<MulBackward0>)\n",
            "x.grad: tensor([ 6., 12.])\n"
          ]
        }
      ],
      "source": [
        "x = tensor([1., 2.], requires_grad=True)\n",
        "print('x:', x)\n",
        "y = 3*x**2\n",
        "print('y:', y)\n",
        "gradient_value = [1., 1.]\n",
        "y.backward(tensor(gradient_value))\n",
        "print('x.grad:', x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmTDzwZTClQR"
      },
      "source": [
        "Now manually calculate Jacobian $J$. In this since $x$ is a vector with components $x_1$ and $x_2$, and $y=3x^2$ is a vector with component $y_1=3x_1^2$ and $y_2=3x_2^2$. We have\n",
        "\n",
        "$$\n",
        "J =  \n",
        "\\left( \\begin{array}{cc}\n",
        "\\frac{\\partial y_1}{\\partial x_1}, \\frac{\\partial y_1}{\\partial x_2} \\\\\n",
        "\\frac{\\partial y_2}{\\partial x_1}, \\frac{\\partial y_2}{\\partial x_2} \\\\\n",
        "\\end{array} \\right)\n",
        "=\n",
        "\\left( \\begin{array}{cc}\n",
        "6x_1, 0 \\\\\n",
        "0, 6x_2 \\\\\n",
        "\\end{array} \\right)\n",
        "$$\n",
        "\n",
        "\n",
        "In numpy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aP25kBZSClQR",
        "outputId": "6deb24b7-89a6-4d73-c913-df05c9455eff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "J:\n",
            "[[ 6.  0.]\n",
            " [ 0. 12.]]\n"
          ]
        }
      ],
      "source": [
        "x = x.detach().numpy()\n",
        "J = array([[6*x[0], 0], [0, 6*x[1]]])\n",
        "print('J:')\n",
        "print(J)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ2hnuHwClQR"
      },
      "source": [
        "In this example, because $y$ is a vector, we must pass a `gradient` argument to `backward()`. We pass $v^T$ with the same length as $y$ and has values 1. We can confirm that $v^T \\cdot J$ gives the same result as `x.grad`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fx6tCbJhClQS",
        "outputId": "a10038dc-5e82-40db-b880-7f8ef8f21753"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vᵀ: [[1. 1.]]\n",
            "vᵀ・J: [[ 6. 12.]]\n"
          ]
        }
      ],
      "source": [
        "vᵀ = array([gradient_value])\n",
        "print('vᵀ:', vᵀ)\n",
        "print('vᵀ・J:', vᵀ@J)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lhS-4krClQS"
      },
      "source": [
        "## input is vector, output is vector, non-one gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXkfPmYhClQS"
      },
      "source": [
        "We can keep everything else the same as above but pass a non-default `gradient` with the value [1, 10] to `backward()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_WxfKzBClQS",
        "outputId": "efae5b63-9b19-4734-ce33-bc620e9e9125"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x: tensor([1., 2.], requires_grad=True)\n",
            "y: tensor([ 3., 12.], grad_fn=<MulBackward0>)\n",
            "x.grad: tensor([  6., 120.])\n"
          ]
        }
      ],
      "source": [
        "x = tensor([1., 2.], requires_grad=True)\n",
        "print('x:', x)\n",
        "y = 3*x**2\n",
        "print('y:', y)\n",
        "gradient_value = [1., 10.]\n",
        "y.backward(tensor(gradient_value))\n",
        "print('x.grad:', x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBND8sN0ClQT"
      },
      "source": [
        "This is the same as setting the value [1,10] for $v^T$, and we can see $v^T \\cdot J$ still matches `x.grad`. Still good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTYve6W_ClQT",
        "outputId": "43cd98d3-98b8-4601-b5e4-e2d5519e4f5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "J:\n",
            "[[ 6.  0.]\n",
            " [ 0. 12.]]\n",
            "vᵀ: [[ 1. 10.]]\n",
            "vᵀ・J: [[  6. 120.]]\n"
          ]
        }
      ],
      "source": [
        "x = x.detach().numpy()\n",
        "J = array([[6*x[0], 0], [0, 6*x[1]]])\n",
        "print('J:')\n",
        "print(J)\n",
        "\n",
        "vᵀ = array([gradient_value])\n",
        "print('vᵀ:', vᵀ)\n",
        "print('vᵀ・J:', vᵀ@J)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApfBxLmeClQT"
      },
      "source": [
        "## input is vector, output is vector - another example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gDQDvQZClQT"
      },
      "source": [
        "Now let's look at a slightly more complex/full-fledged example where both $x=[x_1,x_2]=[1,2]$ is a vector with 2 components and $y=[3x_1^2, x_1^2+2x_2^3, 10x_2]$ is a vector with 3 components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3JJGewUClQT",
        "outputId": "a27d131b-a53c-4ae7-bd10-521a4c0b4ab0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x: tensor([1., 2.], requires_grad=True)\n",
            "y: tensor([ 3., 17., 20.], grad_fn=<CopySlices>)\n",
            "x.grad: tensor([  26., 1240.])\n"
          ]
        }
      ],
      "source": [
        "x = tensor([1., 2.], requires_grad=True)\n",
        "print('x:', x)\n",
        "y = torch.empty(3)\n",
        "y[0] = 3*x[0]**2\n",
        "y[1] = x[0]**2 + 2*x[1]**3\n",
        "y[2] = 10*x[1]\n",
        "print('y:', y)\n",
        "gradient_value = [1., 10., 100.,]\n",
        "y.backward(tensor(gradient_value))\n",
        "print('x.grad:', x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUUvY-F9ClQU"
      },
      "source": [
        "Now manually calculate Jacobian $J$. In this since $x$ is a vector with components $x_1$ and $x_2$, and $y=[y_1=3x_1^2, y_2=x_1^2+2x_2^3, y_3=10x_2]$. We have\n",
        "\n",
        "$$\n",
        "J =  \n",
        "\\left( \\begin{array}{cc}\n",
        "\\frac{\\partial y_1}{\\partial x_1}, \\frac{\\partial y_1}{\\partial x_2} \\\\\n",
        "\\frac{\\partial y_2}{\\partial x_1}, \\frac{\\partial y_2}{\\partial x_2} \\\\\n",
        "\\frac{\\partial y_3}{\\partial x_1}, \\frac{\\partial y_3}{\\partial x_2} \\\\\n",
        "\\end{array} \\right)\n",
        "=\n",
        "\\left( \\begin{array}{cc}\n",
        "6x_1, 0 \\\\\n",
        "2x_1, 6x_2^2 \\\\\n",
        "0,10\n",
        "\\end{array} \\right)\n",
        "$$\n",
        "\n",
        "\n",
        "In numpy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gE9BJYWClQU",
        "outputId": "cecfa55e-d97b-4067-a2a4-c027a5952936"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "J:\n",
            "[[ 6.  0.]\n",
            " [ 2. 24.]\n",
            " [ 0. 10.]]\n"
          ]
        }
      ],
      "source": [
        "x = x.detach().numpy()\n",
        "J = array([[6*x[0], 0],\n",
        "           [2*x[0], 6*x[1]**2],\n",
        "           [0, 10]])\n",
        "print('J:')\n",
        "print(J)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL1O3YOvClQU"
      },
      "source": [
        "In this example, because $y$ is a vector, we must pass a `gradient` argument to `backward()`. We pass $v^T$ with the same length as $y$ and has values [1., 10., 100.,]. We can confirm that $v^T \\cdot J$ gives the same result as `x.grad`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "H2qsn20fClQU",
        "outputId": "20bdab7b-dfc2-4090-f797-8d70ebfb3a8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vᵀ: [[  1.  10. 100.]]\n",
            "vᵀ・J: [[  26. 1240.]]\n"
          ]
        }
      ],
      "source": [
        "vᵀ = array([gradient_value])\n",
        "print('vᵀ:', vᵀ)\n",
        "print('vᵀ・J:', vᵀ@J)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCtvM7gXClQV"
      },
      "source": [
        "## extra cases: broadcast/accumulate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IJB50TvClQV"
      },
      "source": [
        "But there's more. I've not seen it elsewhere other than [here](https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments)), but when $y$ is a scalar, you can actually pass a vector as the `gradient`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRmPJCCxClQV"
      },
      "source": [
        "### input is scalar, output is scalar, gradient is vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gitpcPNdClQV"
      },
      "source": [
        "As in our very first simple example, let $x=1$ and $y = x^2$, both scalar. But instead of a scalar, we can pass a vector of arbitrary length as `gradient`.\n",
        "\n",
        "In pytorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "jMOSR7bkClQV",
        "outputId": "0fe49282-e3d0-42d8-ca87-35d3533ca110"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x: tensor(1., requires_grad=True)\n",
            "y: tensor(1., grad_fn=<PowBackward0>)\n",
            "x.grad: tensor(2222.)\n"
          ]
        }
      ],
      "source": [
        "x = tensor(1., requires_grad=True)\n",
        "print('x:', x)\n",
        "y = x**2\n",
        "print('y:', y)\n",
        "gradient_value = [1., 10., 100., 1000.]\n",
        "y.backward(tensor(gradient_value))\n",
        "print('x.grad:', x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pXF9VjaClQV"
      },
      "source": [
        "With this vector gradient, `backward` accumulates gradient for `x`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GWZ0A3sClQW",
        "outputId": "be134b63-471e-433c-df50-b69ded2b149f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x.grad: tensor(2.)\n",
            "x.grad: tensor(22.)\n",
            "x.grad: tensor(222.)\n",
            "x.grad: tensor(2222.)\n"
          ]
        }
      ],
      "source": [
        "x = tensor(1., requires_grad=True)\n",
        "y = x**2\n",
        "gradient_value = [1., 10., 100., 1000.]\n",
        "for v in gradient_value:\n",
        "    y.backward(tensor(v), retain_graph=True)\n",
        "    print('x.grad:', x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPHKiy5ZClQW"
      },
      "source": [
        "In the matrix multiplication universe, this behavior is as if $J$ is broadcast to the same length of the `gradient`.\n",
        "\n",
        "As before, the Jacobian:\n",
        "$$\n",
        "J = \\left(\\frac{\\partial y_1}{\\partial x_1}\\right) = \\left(\\frac{\\partial y}{\\partial x}\\right) = \\left(2x\\right)\n",
        "$$\n",
        "\n",
        "In numpy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyvIaD3sClQW",
        "outputId": "fa1ea437-23e9-4bb6-c86d-3e056b9c15fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "J: [[2.]]\n",
            "J_broadcast:\n",
            "[[2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]]\n"
          ]
        }
      ],
      "source": [
        "x = x.detach().numpy()\n",
        "J = array([[2*x]])\n",
        "print('J:', J)\n",
        "\n",
        "J_broadcast = np.repeat(J, len(gradient_value), axis=0)\n",
        "print('J_broadcast:')\n",
        "print(J_broadcast)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMds_eiZClQW"
      },
      "source": [
        "We can confirm that $v^T \\cdot J_{broadcast}$ gives the same result as `x.grad`. All good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azqlCLWYClQW",
        "outputId": "35ff2c1e-15d8-4ebe-a316-03f69900742b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vᵀ: [[   1.   10.  100. 1000.]]\n",
            "vᵀ・J_broadcast: [[2222.]]\n"
          ]
        }
      ],
      "source": [
        "vᵀ = array([gradient_value])\n",
        "print('vᵀ:', vᵀ)\n",
        "print('vᵀ・J_broadcast:', vᵀ@J_broadcast)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrccWDSVClQW"
      },
      "source": [
        "### input is vector, output is scalar, gradient is vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vrL1UF8ClQX"
      },
      "source": [
        "Here's another example of broadcast/accumulate. As in our second example, let $x=[x_1,x_2]=[1,2]$ is a vector and $y=\\sum x_i$. But instead of a scalar, we can pass a vector of arbitrary length as `gradient`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svrNUUWEClQX",
        "outputId": "1cc861b5-8d4d-4915-9625-9c86b1567006"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x: tensor([1., 2.], requires_grad=True)\n",
            "y: tensor(3., grad_fn=<AddBackward0>)\n",
            "x.grad: tensor([1111., 1111.])\n"
          ]
        }
      ],
      "source": [
        "x = tensor([1., 2.], requires_grad=True)\n",
        "print('x:', x)\n",
        "y = sum(x)\n",
        "print('y:', y)\n",
        "gradient_value = [1., 10., 100., 1000.]\n",
        "y.backward(tensor(gradient_value))\n",
        "print('x.grad:', x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRM_JcNzClQX"
      },
      "source": [
        "With this vector gradient, `backward` accumulates gradient for `x`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynVBgZDUClQX",
        "outputId": "4ea458a0-1091-4594-f71c-e05325ac3b6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x.grad: tensor([1., 1.])\n",
            "x.grad: tensor([11., 11.])\n",
            "x.grad: tensor([111., 111.])\n",
            "x.grad: tensor([1111., 1111.])\n"
          ]
        }
      ],
      "source": [
        "x = tensor([1., 2.], requires_grad=True)\n",
        "y = sum(x)\n",
        "gradient_value = [1., 10., 100., 1000.]\n",
        "for v in gradient_value:\n",
        "    y.backward(tensor(v), retain_graph=True)\n",
        "    print('x.grad:', x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFkUPswPClQX"
      },
      "source": [
        "In the matrix multiplication universe, this behavior is as if $J$ is broadcast to the same length of the `gradient`.\n",
        "\n",
        "$$\n",
        "J =  \n",
        "\\left( \\begin{array}{c}\n",
        "\\frac{\\partial y_1}{\\partial x_1}, \\frac{\\partial y_1}{\\partial x_2}\n",
        "\\end{array} \\right)\n",
        "=\n",
        "\\left( \\begin{array}{c} 1,1\n",
        "\\end{array} \\right)\n",
        "$$\n",
        "\n",
        "\n",
        "In numpy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0S9diRkPClQY",
        "outputId": "7b27766d-6bcc-40b7-ad52-3dc8273e58bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "J: [[1 1]]\n",
            "J_broadcast:\n",
            "[[1 1]\n",
            " [1 1]\n",
            " [1 1]\n",
            " [1 1]]\n"
          ]
        }
      ],
      "source": [
        "x = x.detach().numpy()\n",
        "J = array([[1, 1]])\n",
        "print('J:', J)\n",
        "\n",
        "J_broadcast = np.repeat(J, len(gradient_value), axis=0)\n",
        "print('J_broadcast:')\n",
        "print(J_broadcast)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBWJD_C8ClQY"
      },
      "source": [
        "We can confirm that $v^T \\cdot J_{broadcast}$ gives the same result as `x.grad`. All good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOJl0IK_ClQY",
        "outputId": "ce1f494e-c429-4e63-d1e2-69f70166a164"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vᵀ: [[   1.   10.  100. 1000.]]\n",
            "vᵀ・J_broadcast: [[1111. 1111.]]\n"
          ]
        }
      ],
      "source": [
        "vᵀ = array([gradient_value])\n",
        "print('vᵀ:', vᵀ)\n",
        "print('vᵀ・J_broadcast:', vᵀ@J_broadcast)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCdV6zAZClQY"
      },
      "source": [
        "## That's it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOyR3FgzClQY"
      },
      "source": [
        "Hope this helps. Since there's some manual calculation of gradients here, if I made mistakes, please let me know so I can correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5yBHsLqClQY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "fastaiv1-cpu",
      "language": "python",
      "name": "fastaiv1-cpu"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}